{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce01ce76-b3e7-4d5f-bd7d-a60ff4f8de75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "with open('msr_paraphrase_train.txt', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        split_line = line.strip().split('\\t')\n",
    "        if len(split_line) == 5:\n",
    "            data.append(split_line)\n",
    "        else:\n",
    "            print(f\"Skipping line due to incorrect number of columns: {line}\")\n",
    "\n",
    "columns = [\"Quality\", \"#1 ID\", \"#2 ID\", \"#1 String\", \"#2 String\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df['Quality'] = df['Quality'].astype(int)\n",
    "\n",
    "data = []\n",
    "with open('msr_paraphrase_test.txt', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        split_line = line.strip().split('\\t')\n",
    "        if len(split_line) == 5:\n",
    "            data.append(split_line)\n",
    "        else:\n",
    "            print(f\"Skipping line due to incorrect number of columns: {line}\")\n",
    "\n",
    "columns = [\"Quality\", \"#1 ID\", \"#2 ID\", \"#1 String\", \"#2 String\"]\n",
    "df_test = pd.DataFrame(data, columns=columns)\n",
    "df_test['Quality'] = df_test['Quality'].astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831399d-b56f-4bdd-bd82-b1af8d1bb4e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc7393-bea9-41d8-9bcf-a6a14ce9f5a6",
   "metadata": {},
   "source": [
    "- BERT is a pre-trained transformer model designed to understand the context of words in a sentence. It captures intricate patterns and relationships within the text through its deep architecture.\n",
    "    - Instead of manual feature engineering, BERT learns the features directly from the text during the training process. This allows BERT to capture complex linguistic nuances that are hard to engineer manually.\n",
    "- BERT embeddings are high-dimensional vectors that represent the meaning of sentences. When fine-tuning BERT, we leverage these embeddings to directly classify paraphrase pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b39c4e-b4d5-482e-b9dd-aba5dec06929",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d9086d-06d6-4638-a381-03281215a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "df['#1 String Cleaned'] = df['#1 String'].apply(clean_text)\n",
    "df['#2 String Cleaned'] = df['#2 String'].apply(clean_text)\n",
    "\n",
    "# Applying stop words removal and lemmatization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word.lower() not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['#1 String Processed'] = df['#1 String Cleaned'].apply(preprocess_text)\n",
    "df['#2 String Processed'] = df['#2 String Cleaned'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40681611-b9e8-4c5e-afac-6f0f81d61431",
   "metadata": {},
   "source": [
    "## ParaphraseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49dcbd6a-cc38-4f24-858a-2ee734d146fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for loading the data\n",
    "class ParaphraseDataset(Dataset):\n",
    "    ''' \n",
    "    This tells Python that ParaphraseDataset is a type of Dataset. \n",
    "    Dataset is a class from PyTorch's torch.utils.data module that \n",
    "    helps to manage and preprocess data. \n",
    "    '''\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    '''\n",
    "    This is the initializer method. It runs when we create an instance of the class.\n",
    "    df: The DataFrame containing the data.\n",
    "    tokenizer: The tokenizer to convert text into tokens.\n",
    "    max_len: The maximum length for the tokenized sequences.\n",
    "    self.df = df: Saves the DataFrame as an attribute of the class.\n",
    "    self.tokenizer = tokenizer: Saves the tokenizer as an attribute of the class.\n",
    "    self.max_len = max_len: Saves the maximum length as an attribute of the class.\n",
    "\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text1 = row['#1 String Processed']\n",
    "        text2 = row['#2 String Processed']\n",
    "        label = int(row['Quality'])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text1,\n",
    "            text2,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        '''\n",
    "        encode_plus: Encodes the text into token IDs, \n",
    "        adds special tokens (e.g., [CLS], [SEP]), pads or truncates the sequence to max_len, \n",
    "        and returns the result as PyTorch tensors ('pt').\n",
    "        \n",
    "        Why: Converts text into a format that BERT can process, \n",
    "        ensuring all sequences have the same length and special tokens are added.\n",
    "        '''\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    '''\n",
    "    Dictionary: Returns input IDs, attention masks, and labels.\n",
    "\n",
    "    Why: Organizes the tokenized data and labels in a way that the DataLoader can easily use.\n",
    "    '''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1034fb9-d37e-4869-ab22-ddd70a188530",
   "metadata": {},
   "source": [
    "**BERT input components**\n",
    "1. input_ids:\n",
    "    - The token IDs representing the input text. These IDs correspond to words or sub-words in the text, which have been converted into numerical form using the BERT tokenizer.\n",
    "    - Why it's needed: BERT processes text by converting it into token IDs. This numerical representation allows the model to perform computations on the text data.\n",
    "        - Essential for feeding textual data into the BERT model, as the model operates on these numerical IDs.\n",
    "2. attention_mask:\n",
    "    - A binary mask that indicates which tokens should be attended to and which should be ignored (typically padding tokens). It is an array of 1s and 0s where 1 indicates a token to be attended to and 0 indicates a padding token.\n",
    "    - Why it's needed: BERT uses the attention mechanism to focus on relevant tokens in the input sequence. The attention mask helps the model distinguish between actual data tokens and padding tokens, ensuring that the padding tokens do not influence the output.\n",
    "        - Ensure that only meaningful tokens are attended to during the model's computations.\n",
    "\n",
    "3. labels:\n",
    "    - The actual class labels for the input text pairs (e.g., whether two sentences are paraphrases or not). In this context, it's a binary label (0 or 1).\n",
    "    - Why it's needed: During training, the labels are used to compute the loss by comparing the model's predictions against the true labels. This loss is then used to update the model parameters to improve its predictions.\n",
    "        - Crucial for supervised learning, as the loss calculated using these labels guides the optimization process, helping the model learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d80c75-e9ae-4747-8f19-7a752306b17c",
   "metadata": {},
   "source": [
    "## BERT tokenizer and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb460aba-73db-4981-8fe8-57dbe7158763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataLoader is a PyTorch class that helps to load data in batches. \\n- Batching: Grouping the data into smaller chunks (batches) that are fed to the model sequentially.\\n- Shuffling: Randomizing the order of the data to prevent the model from learning the order.\\n- Parallel Loading: Loading data using multiple CPU cores to speed up the process.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "'''Tokenize the text data and create DataLoader objects for batching and shuffling.'''\n",
    "\n",
    "\n",
    "# Dataset Creation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = ParaphraseDataset(train_df, tokenizer, max_len=128)\n",
    "val_dataset = ParaphraseDataset(val_df, tokenizer, max_len=128)\n",
    "'''\n",
    "Ensures that data is tokenized and prepared correctly for both training and validation\n",
    "\n",
    "When you see ParaphraseDataset(train_df, tokenizer, max_len=128), \n",
    "you are creating an instance of ParaphraseDataset with these specific parameters. \n",
    "- The class Dataset that ParaphraseDataset inherits from is not a parameter but a base \n",
    "    class providing some functionality.\n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "'''\n",
    "DataLoader is a PyTorch class that helps to load data in batches. \n",
    "- Batching: Grouping the data into smaller chunks (batches) that are fed to the model sequentially.\n",
    "- Shuffling: Randomizing the order of the data to prevent the model from learning the order.\n",
    "- Parallel Loading: Loading data using multiple CPU cores to speed up the process.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215fb2d7-8692-4c58-b50a-abf8237acd1d",
   "metadata": {},
   "source": [
    "## Paraphrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9363aadb-ce40-42a0-a7d6-8f2d1682f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nCriterion:\\n- criterion = nn.CrossEntropyLoss(): Defines the loss function. \\n- Cross-entropy loss is used for classification tasks. \\n- It measures the difference between the predicted probabilities and the true labels.\\n\\nOptimizer:\\n- optimizer = optim.AdamW(model.parameters(), lr=2e-5): Defines the optimizer. \\n- AdamW is a variant of the Adam optimizer that includes weight decay to reduce overfitting. \\n- It updates the model's parameters to minimize the loss.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the BERT model\n",
    "class ParaphraseModel(nn.Module):\n",
    "    '''\n",
    "    - Defines a new class ParaphraseModel that inherits from nn.Module.\n",
    "    - nn.Module is a base class for all neural network modules in PyTorch. \n",
    "    It provides a way to define, manage, and operate on neural network layers and parameters.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ParaphraseModel, self).__init__()\n",
    "        '''\n",
    "        Calls the constructor of the parent class (nn.Module), \n",
    "        which is necessary to properly initialize the module.\n",
    "        '''\n",
    "        \n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "        '''\n",
    "        BertForSequenceClassification: A class from the Hugging Face Transformers library \n",
    "        specifically designed for sequence classification tasks.\n",
    "        - num_labels=2: Sets the number of output labels to 2, which is suitable for binary classification (paraphrase or not).\n",
    "        \n",
    "        When you use the BertForSequenceClassification class with the from_pretrained method, \n",
    "        you are essentially taking the pre-trained BERT model (bert-base-uncased) \n",
    "        and adding a classification head to it.\n",
    "        - The pre-trained BERT model (bert-base-uncased) \n",
    "            is highly effective at understanding language \n",
    "            but does not directly provide outputs suitable for specific tasks like classification.\n",
    "        '''\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs.loss, outputs.logits\n",
    "        '''\n",
    "           input_ids: Token IDs of the input text.\n",
    "            attention_mask: is a tensor that indicates which tokens in the input should \n",
    "            be attended to and which should be ignored (typically padding tokens). \n",
    "            It is an integral part of the input when working with models like BERT that\n",
    "            use attention mechanisms to process sequences of text.\n",
    "\n",
    "            - Padded tokens are special tokens added to text sequences to ensure that \n",
    "                all sequences in a batch have the same length. \n",
    "            - This is necessary because many machine learning models, including BERT, \n",
    "                require inputs of uniform size for efficient processing, \n",
    "                especially when using mini-batch training.\n",
    "                - The attention mask has the same length as the sequence and \n",
    "                    indicates which tokens are padding (typically with a value of 0) \n",
    "                    and which are actual data tokens (typically with a value of 1).\n",
    "            labels: True labels for the data.\n",
    "            Outputs: When you pass these to the BERT model, it returns:\n",
    "            loss: The classification loss if labels are provided.\n",
    "            logits: The raw, unnormalized scores for each class.        \n",
    "        '''\n",
    "        \n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ParaphraseModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "'''\n",
    "Criterion:\n",
    "- criterion = nn.CrossEntropyLoss(): Defines the loss function. \n",
    "- Cross-entropy loss is used for classification tasks. \n",
    "- It measures the difference between the predicted probabilities and the true labels.\n",
    "\n",
    "Optimizer:\n",
    "- optimizer = optim.AdamW(model.parameters(), lr=2e-5): Defines the optimizer. \n",
    "- AdamW is a variant of the Adam optimizer that includes weight decay to reduce overfitting. \n",
    "- It updates the model's parameters to minimize the loss.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e103f1a-0894-493f-ab20-edd9ebac7975",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8175fb-1760-4a15-92df-66918412fa57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        '''\n",
    "        for data in tqdm(data_loader): Iterates over batches of data provided by data_loader. \n",
    "        tqdm is a library that shows a progress bar during the loop.\n",
    "        '''\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "        # Moves these to the device used to run the model\n",
    "\n",
    "        \n",
    "        # 1. Zeroing Gradients\n",
    "        optimizer.zero_grad()\n",
    "        '''\n",
    "        - Gradients indicate how much a small change in each parameter will affect the loss.\n",
    "        - During backpropagation, gradients are computed for each parameter to update \n",
    "        them in the direction that minimizes the loss.\n",
    "\n",
    "        - By default, PyTorch accumulates gradients. \n",
    "        This means that when you call the backward() method to compute gradients, \n",
    "        the gradients for each parameter are added to the existing gradients \n",
    "        from previous iterations.\n",
    "            - Why Accumulation?: Accumulating gradients can be useful in some cases, \n",
    "                such as when implementing certain types of optimization algorithms \n",
    "                or performing gradient accumulation over multiple mini-batches \n",
    "                to simulate larger batch sizes.\n",
    "\n",
    "        - In most standard training loops, you do not want gradients to accumulate across \n",
    "        iterations. \n",
    "            - Instead, you want the gradients to reflect only the current mini-batch of data.\n",
    "            - If you do not zero the gradients, they will accumulate, \n",
    "                leading to incorrect gradient values and potentially \n",
    "                causing the model to converge poorly or diverge.\n",
    "                \n",
    "        - This allows the optimizer to update the parameters correctly based on\n",
    "        the current batch's contribution to the loss.\n",
    "        '''\n",
    "        \n",
    "        # 2. Forward Pass\n",
    "        loss, logits = model(input_ids, attention_mask, labels)\n",
    "        '''\n",
    "        - Logits: The model produces raw, unnormalized scores for each class, known as logits.\n",
    "        - Loss: If labels are provided, the model also calculates the loss, \n",
    "        which is a measure of how well the model's predictions match the true labels. \n",
    "            - This loss will later be used for backpropagation.\n",
    "        '''\n",
    "                \n",
    "        \n",
    "        # Predictions\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        '''\n",
    "        This step converts the logits into actual class predictions.\n",
    "        \n",
    "        The torch.max function returns two tensors:\n",
    "        1. The first tensor (which is assigned to _) contains the maximum scores \n",
    "        for each sample. This is often not needed when you only care about the \n",
    "        indices of the maximum values, hence it's assigned to _ (a common Python \n",
    "        convention for ignoring values).\n",
    "        2. The second tensor (assigned to preds) contains the indices of the\n",
    "        maximum scores, which correspond to the predicted classes.\n",
    "        '''\n",
    "\n",
    "        # 3. Backward Pass\n",
    "        loss.backward()\n",
    "        '''\n",
    "        Computes the gradients of the loss with respect to each model parameter \n",
    "        (weight and bias) using backpropagation.\n",
    "        '''\n",
    "        \n",
    "        # 4. Optimizer Step\n",
    "        optimizer.step()\n",
    "        '''\n",
    "        Updates the model parameters using the computed gradients.\n",
    "        \n",
    "        The optimizer adjusts the model parameters (weights and biases) \n",
    "        based on the gradients computed during the backward pass. \n",
    "            - This adjustment is typically done using optimization algorithms \n",
    "            like Stochastic Gradient Descent (SGD) or Adam.\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        losses.append(loss.item())\n",
    "        '''\n",
    "        Stores the loss for the current batch.\n",
    "        The item() method extracts the scalar value from this tensor. \n",
    "            - In PyTorch, item() is used to get a standard Python number \n",
    "            from a tensor with a single value. \n",
    "            - This is useful for logging or aggregating purposes.\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        '''\n",
    "        Where do labels come from? \n",
    "            - In each iteration of the training loop, \n",
    "            the DataLoader provides a batch of data, which includes both inputs \n",
    "            (such as input_ids and attention_mask) and labels.\n",
    "        \n",
    "        Counts correct predictions for accuracy calculation.\n",
    "        '''\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae7e4c1-2bb7-4083-98da-fe7a5651a56a",
   "metadata": {},
   "source": [
    "**Order of Pytorch training**\n",
    "\n",
    "1. Gradient Accumulation: Gradients accumulate by default in PyTorch, so optimizer.zero_grad() is called before the forward pass to ensure gradients are zeroed for the current batch.\n",
    "2. Forward Pass First: The forward pass must occur before backpropagation because gradients are calculated based on the loss computed in the forward pass.\n",
    "    - Propagates input data through the network to produce an output. It is used to compute the predictions and the loss.\n",
    "3. Backward Pass After Forward: Backpropagation computes gradients after the forward pass, based on the loss\n",
    "    - Computes gradients of the loss with respect to the model parameters using backpropagation. These gradients are then used by the optimizer to update the parameters.\n",
    "4. Optimizer Step Last: The optimizer updates the model parameters after gradients are computed and before moving on to the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcae4d61-7e10-44b5-84c2-8e276904b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval\n",
    "    # Puts the model in evaluation mode. \n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        '''\n",
    "        This context manager disables gradient computation. During evaluation, \n",
    "        we don't need to compute gradients, which saves memory and computational resources.\n",
    "        '''\n",
    "        for data in data_loader:\n",
    "            '''\n",
    "            Loops over each batch of data provided by the DataLoader. \n",
    "            The DataLoader handles batching, shuffling, and parallel data loading.\n",
    "            '''\n",
    "            \n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            \n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b6fb7-8598-45a6-82fe-62f22fdcafbf",
   "metadata": {},
   "source": [
    "### Training vs Evaluation\n",
    "Training Phase:\n",
    "- Objective: Adjust the model parameters to minimize the loss and improve performance.\n",
    "- Operations:\n",
    "- Forward Pass: Compute the output (predictions) and loss.\n",
    "- Backward Pass: Calculate gradients of the loss with respect to the model parameters using backpropagation.\n",
    "- Optimization: Update the model parameters to reduce the loss using an optimizer (e.g., Adam, SGD).\n",
    "\n",
    "Evaluation Phase:\n",
    "- Objective: Assess the model's performance on unseen data without updating the model parameters.\n",
    "Operations:\n",
    "- Forward Pass: Compute the output (predictions) and loss.\n",
    "- No Backward Pass: No gradients are computed.\n",
    "- No Optimization: Model parameters are not updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35504bb6-6255-437b-92fb-02ab62c62163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [01:54<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5725470986582485, Train accuracy: 0.6981595092024541\n",
      "Validation loss: 0.4764804974490521, Validation accuracy: 0.7683823529411764\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [01:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4103083444722727, Train accuracy: 0.8119631901840492\n",
      "Validation loss: 0.4473759853372387, Validation accuracy: 0.7769607843137255\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [01:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2333682452638944, Train accuracy: 0.9085889570552148\n",
      "Validation loss: 0.5452328455798766, Validation accuracy: 0.7598039215686274\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade torch torchvision torchaudio\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    # Use train_loader to train\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')\n",
    "\n",
    "    # Use val_loader to test\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, device)\n",
    "    print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb95c27-8c6c-448b-8021-30cc286752a1",
   "metadata": {},
   "source": [
    "Loss is a metric that quantifies how well a model’s predictions match the actual labels. It measures the difference between the predicted outputs and the true outputs.\n",
    "\n",
    "Lower loss values generally indicate better model performance as it means the predictions are closer to the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1408f-2567-4b15-8785-54898ed431ad",
   "metadata": {},
   "source": [
    "# Code in one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41ca318-8b3c-485a-8f6f-f5c3b4644f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [01:53<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5770857813311558, Train accuracy: 0.6806748466257669\n",
      "Validation loss: 0.4743262202131982, Validation accuracy: 0.7708333333333334\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [01:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.44783832059771406, Train accuracy: 0.7889570552147239\n",
      "Validation loss: 0.4705883343430126, Validation accuracy: 0.7512254901960784\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [01:53<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2910658292947154, Train accuracy: 0.8812883435582822\n",
      "Validation loss: 0.5576268259216758, Validation accuracy: 0.7745098039215687\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "torch.cuda.empty_cache()\n",
    "# Loading data\n",
    "data = []\n",
    "with open('msr_paraphrase_train.txt', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        split_line = line.strip().split('\\t')\n",
    "        if len(split_line) == 5:\n",
    "            data.append(split_line)\n",
    "        else:\n",
    "            print(f\"Skipping line due to incorrect number of columns: {line}\")\n",
    "\n",
    "columns = [\"Quality\", \"#1 ID\", \"#2 ID\", \"#1 String\", \"#2 String\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df['Quality'] = df['Quality'].astype(int)\n",
    "\n",
    "data = []\n",
    "with open('msr_paraphrase_test.txt', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        split_line = line.strip().split('\\t')\n",
    "        if len(split_line) == 5:\n",
    "            data.append(split_line)\n",
    "        else:\n",
    "            print(f\"Skipping line due to incorrect number of columns: {line}\")\n",
    "\n",
    "columns = [\"Quality\", \"#1 ID\", \"#2 ID\", \"#1 String\", \"#2 String\"]\n",
    "df_test = pd.DataFrame(data, columns=columns)\n",
    "df_test['Quality'] = df_test['Quality'].astype(int)\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "df['#1 String Cleaned'] = df['#1 String'].apply(clean_text)\n",
    "df['#2 String Cleaned'] = df['#2 String'].apply(clean_text)\n",
    "\n",
    "# Applying stop words removal and lemmatization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word.lower() not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['#1 String Processed'] = df['#1 String Cleaned'].apply(preprocess_text)\n",
    "df['#2 String Processed'] = df['#2 String Cleaned'].apply(preprocess_text)\n",
    "\n",
    "# Define a custom dataset for loading the data\n",
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text1 = row['#1 String Processed']\n",
    "        text2 = row['#2 String Processed']\n",
    "        label = int(row['Quality'])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text1,\n",
    "            text2,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = ParaphraseDataset(train_df, tokenizer, max_len=128)\n",
    "val_dataset = ParaphraseDataset(val_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Define the BERT model\n",
    "class ParaphraseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParaphraseModel, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs.loss, outputs.logits\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ParaphraseModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = model(input_ids, attention_mask, labels)\n",
    "        \n",
    "        #predictions\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Train loss: {train_loss}, Train accuracy: {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model, val_loader, criterion, device)\n",
    "    print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}')\n",
    "\n",
    "torch.save(model.state_dict(), \"best_model_BERT.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096bf7b6-aad2-4461-88c2-e422160b8e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
