{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 255/255 [00:24<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.8374, 'train_samples_per_second': 164.108, 'train_steps_per_second': 10.267, 'train_loss': 0.1707887088551241, 'epoch': 1.0}\n",
      "Accuracy: 0.7559420289855072\n",
      "F1 Score: 0.8404698749526336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Accuracy: 0.7559420289855072\n",
      "Loaded Model F1 Score: 0.8404698749526336\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence-transformers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load your dataset\n",
    "def load_dataset(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            split_line = line.strip().split('\\t')\n",
    "            if len(split_line) == 5:\n",
    "                data.append(split_line)\n",
    "    columns = [\"Quality\", \"#1 ID\", \"#2 ID\", \"#1 String\", \"#2 String\"]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df['Quality'] = df['Quality'].astype(int)\n",
    "    return df\n",
    "\n",
    "df_train = load_dataset('msr_paraphrase_train.txt')\n",
    "df_test = load_dataset('msr_paraphrase_test.txt')\n",
    "\n",
    "# Prepare the dataset for training\n",
    "train_samples = []\n",
    "for i in range(len(df_train)):\n",
    "    train_samples.append(InputExample(texts=[df_train.iloc[i]['#1 String'], df_train.iloc[i]['#2 String']], label=int(df_train.iloc[i]['Quality'])))\n",
    "\n",
    "test_samples = []\n",
    "for i in range(len(df_test)):\n",
    "    test_samples.append(InputExample(texts=[df_test.iloc[i]['#1 String'], df_test.iloc[i]['#2 String']], label=int(df_test.iloc[i]['Quality'])))\n",
    "\n",
    "# Load the SBERT model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Prepare the dataloader\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, test_samples):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for sample in test_samples:\n",
    "        embeddings = model.encode(sample.texts)\n",
    "        cosine_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
    "        pred = 1 if cosine_sim > 0.5 else 0  # Threshold of 0.5 for similarity\n",
    "        predictions.append(pred)\n",
    "        labels.append(sample.label)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return accuracy, f1\n",
    "\n",
    "accuracy, f1 = evaluate_model(model, test_samples)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Save the model\n",
    "model_save_path = 'best_model_sbert.bin'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# Load the model\n",
    "loaded_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Verify the loaded model\n",
    "accuracy, f1 = evaluate_model(loaded_model, test_samples)\n",
    "print(f'Loaded Model Accuracy: {accuracy}')\n",
    "print(f'Loaded Model F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://c0d34fda03feca8e91.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c0d34fda03feca8e91.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence-transformers gradio\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load the model state dictionary if you have a saved model\n",
    "model_save_path = 'best_model_sbert.bin'\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to compare texts using the SBERT model\n",
    "def compare_texts(text1, text2, model):\n",
    "    embeddings = model.encode([text1, text2], convert_to_tensor=True)\n",
    "    embeddings = embeddings.cpu()  # Move embeddings to CPU\n",
    "    cosine_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
    "    return cosine_sim\n",
    "\n",
    "# Function to update the dataframe with text numbers\n",
    "def update_dataframe(df):\n",
    "    df[\"Number\"] = range(1, len(df) + 1)\n",
    "    return df\n",
    "\n",
    "# Function to determine perplexity for t-SNE based on number of samples\n",
    "def determine_perplexity(num_samples):\n",
    "    if num_samples < 10:\n",
    "        return 2\n",
    "    elif num_samples < 50:\n",
    "        return 5\n",
    "    elif num_samples < 100:\n",
    "        return 10\n",
    "    elif num_samples < 500:\n",
    "        return 30\n",
    "    else:\n",
    "        return 50\n",
    "\n",
    "# Function to check similarity and generate clusters\n",
    "def check_similarity(df):\n",
    "    texts = df[\"Answers\"].tolist()\n",
    "    n = len(texts)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            score = compare_texts(texts[i], texts[j], model)\n",
    "            similarity_matrix[i][j] = score\n",
    "            similarity_matrix[j][i] = score\n",
    "\n",
    "    clustering = AgglomerativeClustering(metric='precomputed', linkage='average', n_clusters=None, distance_threshold=0.5)\n",
    "    clusters = clustering.fit_predict(1 - similarity_matrix)  # Convert similarity to distance\n",
    "\n",
    "    cluster_texts = {i: [] for i in np.unique(clusters)}\n",
    "    cluster_scores = {i: [] for i in np.unique(clusters)}\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        cluster_texts[cluster_id].append(f\"Text {idx + 1}\")\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if clusters[i] == clusters[j]:\n",
    "                cluster_scores[clusters[i]].append(similarity_matrix[i][j])\n",
    "\n",
    "    cluster_data = {\n",
    "        \"Cluster\": [],\n",
    "        \"Texts\": [],\n",
    "        \"Average Similarity\": []\n",
    "    }\n",
    "\n",
    "    for cluster_id, texts in cluster_texts.items():\n",
    "        cluster_data[\"Cluster\"].append(cluster_id)\n",
    "        cluster_data[\"Texts\"].append(\", \".join(texts))\n",
    "        cluster_data[\"Average Similarity\"].append(np.mean(cluster_scores[cluster_id]))\n",
    "\n",
    "    perplexity = determine_perplexity(n)\n",
    "\n",
    "    tsne = TSNE(n_components=2, metric=\"precomputed\", perplexity=perplexity, init='random')\n",
    "    tsne_results = tsne.fit_transform(1 - similarity_matrix)  # Convert similarity to distance\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    scatter = ax.scatter(tsne_results[:, 0], tsne_results[:, 1], c=clusters, cmap='viridis')\n",
    "    for i, text in enumerate([f\"Text {i + 1}\" for i in range(n)]):\n",
    "        ax.annotate(text, (tsne_results[i, 0], tsne_results[i, 1]))\n",
    "    plt.title(\"t-SNE Visualization of Text Clusters\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.colorbar(scatter)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"tsne_clusters.png\")\n",
    "    \n",
    "    return pd.DataFrame(cluster_data), \"tsne_clusters.png\"\n",
    "\n",
    "# HTML and markdown content for the interface\n",
    "intro_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>Paraphrase Detection Project</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>Welcome to the Paraphrase Detection Project</h1>\n",
    "<p>My name is Sangjun Ko, and this project is part of my work as a member of the ATLAS Machine Learning Team at the University of Illinois Urbana-Champaign (UIUC).</p>\n",
    "<p>This project focuses on detecting paraphrases using advanced Natural Language Processing (NLP) techniques. I use a pre-trained SBERT model to analyze and compare different text inputs to determine if they are paraphrases of each other.</p>\n",
    "<h2>About NLP and Paraphrase Detection</h2>\n",
    "<p>Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both valuable and meaningful.</p>\n",
    "<p>Paraphrase detection is a specific task in NLP where the goal is to determine if two pieces of text have the same meaning but are expressed differently. This can be useful in various applications such as plagiarism detection, information retrieval, and question-answering systems.</p>\n",
    "<h2>Methods Used</h2>\n",
    "<ul>\n",
    "<li><strong>SBERT Model:</strong> A Sentence-BERT model for NLP tasks.</li>\n",
    "<li><strong>Hugging Face Transformers:</strong> I leverage the Transformers library by Hugging Face, which provides pre-trained models and tools for various NLP tasks.</li>\n",
    "<li><strong>Dataset Source:</strong> The dataset used for training the model is sourced from the <a href=\"https://www.microsoft.com/en-us/download/details.aspx?id=52398\" target=\"_blank\">Microsoft Paraphrase Corpus</a>.</li>\n",
    "</ul>\n",
    "<h2>Learn More</h2>\n",
    "<p>If you are interested in learning more about NLP and how to build projects like these, here are some resources:</p>\n",
    "<ul>\n",
    "<li><a href=\"https://www.coursera.org/specializations/natural-language-processing\" target=\"_blank\">Coursera: Natural Language Processing</a></li>\n",
    "<li><a href=\"https://huggingface.co/transformers/\" target=\"_blank\">Hugging Face Transformers Documentation</a></li>\n",
    "<li><a href=\"https://arxiv.org/abs/1907.11692\" target=\"_blank\">SBERT: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a></li>\n",
    "</ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "instructions = \"\"\"\n",
    "## Instructions\n",
    "1. Enter the texts you want to compare in the \"Input DataFrame\" below. \n",
    "2. Each row represents a different text.\n",
    "3. After entering your texts, click the \"Check for Similarity\" button.\n",
    "4. The results will show the identified clusters and a visualization of the text similarities.\n",
    "\"\"\"\n",
    "\n",
    "description = \"\"\"\n",
    "## Interpretation Guide\n",
    "\n",
    "### Results DataFrame:\n",
    "The \"Paraphrase Check Results\" table lists the clusters identified among the input texts. Each row in the table represents a cluster, and the \"Texts\" column lists the text numbers that belong to that cluster.\n",
    "\n",
    "### Similarity Scores:\n",
    "The similarity between texts is calculated pairwise, with scores ranging from 0 to 1. A higher score indicates a higher probability that the texts are paraphrases of each other.\n",
    "\n",
    "### Cluster Visualization:\n",
    "The t-SNE visualization plot provides a graphical representation of the clusters. Each point corresponds to a text, and points that are closer together represent texts that are more similar to each other. The color indicates the cluster to which each text belongs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"Introduction\"):\n",
    "            gr.HTML(intro_html)\n",
    "        with gr.Tab(\"Paraphrase Detection\"):\n",
    "            gr.Markdown(\"# Paraphrase Detection Interface\")\n",
    "            gr.Markdown(instructions)\n",
    "            \n",
    "            data = {\n",
    "                \"Number\": [1, 2, 3, 4, 5],\n",
    "                \"Answers\": [\"\"] * 5  \n",
    "            }\n",
    "            dataframe_input = gr.Dataframe(value=pd.DataFrame(data), label=\"Input DataFrame\")\n",
    "\n",
    "            btn_check_similarity = gr.Button(\"Check for Similarity\")\n",
    "\n",
    "            gr.Markdown(description)\n",
    "            \n",
    "            results_output = gr.Dataframe(label=\"Paraphrase Check Results\")\n",
    "            image_output = gr.Image(label=\"Cluster Visualization\")\n",
    "\n",
    "            dataframe_input.change(fn=update_dataframe, inputs=dataframe_input, outputs=dataframe_input)\n",
    "\n",
    "            btn_check_similarity.click(\n",
    "                fn=check_similarity, \n",
    "                inputs=dataframe_input, \n",
    "                outputs=[results_output, image_output]\n",
    "            )\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
