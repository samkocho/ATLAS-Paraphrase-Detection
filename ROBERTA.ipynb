{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b983d53-6533-4d1b-9719-1edaff306638",
   "metadata": {},
   "source": [
    "# week 9 stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a062602-feb0-429f-821b-0f9a5cc3c067",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:54<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6382749781889074, Train accuracy: 0.6236196319018406, Train F1: 0.7587020648967552\n",
      "Validation loss: 0.6134999829764459, Validation accuracy: 0.7267156862745098, Validation F1: 0.8332086761406132\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:52<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5902815706589642, Train accuracy: 0.6460122699386504, Train F1: 0.7529965753424658\n",
      "Validation loss: 0.49389328325496, Validation accuracy: 0.7316176470588235, Validation F1: 0.8332063975628331\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:53<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5405519359252032, Train accuracy: 0.6990797546012271, Train F1: 0.7761806981519507\n",
      "Validation loss: 0.47818009219333235, Validation accuracy: 0.767156862745098, Validation F1: 0.843492586490939\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:53<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.45951179897083955, Train accuracy: 0.7539877300613498, Train F1: 0.8079501915708812\n",
      "Validation loss: 0.5096894225069121, Validation accuracy: 0.7267156862745098, Validation F1: 0.7802955665024631\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:53<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3733238799899232, Train accuracy: 0.8006134969325154, Train F1: 0.8461902508282063\n",
      "Validation loss: 0.5156420529180882, Validation accuracy: 0.7549019607843137, Validation F1: 0.8106060606060606\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:52<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3009348394824009, Train accuracy: 0.8279141104294478, Train F1: 0.863934028619937\n",
      "Validation loss: 0.5827327191318367, Validation accuracy: 0.7977941176470588, Validation F1: 0.8595744680851063\n",
      "Early stopping!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Loading data\n",
    "data = []\n",
    "with open('msr_paraphrase_train.txt', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        split_line = line.strip().split('\\t')\n",
    "        if len(split_line) == 5:\n",
    "            data.append(split_line)\n",
    "        else:\n",
    "            print(f\"Skipping line due to incorrect number of columns: {line}\")\n",
    "\n",
    "columns = [\"Quality\", \"#1 ID\", \"#2 ID\", \"#1 String\", \"#2 String\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df['Quality'] = df['Quality'].astype(int)\n",
    "\n",
    "data = []\n",
    "with open('msr_paraphrase_test.txt', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        split_line = line.strip().split('\\t')\n",
    "        if len(split_line) == 5:\n",
    "            data.append(split_line)\n",
    "        else:\n",
    "            print(f\"Skipping line due to incorrect number of columns: {line}\")\n",
    "\n",
    "df_test = pd.DataFrame(data, columns=columns)\n",
    "df_test['Quality'] = df_test['Quality'].astype(int)\n",
    "\n",
    "# Clean and preprocess data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text_advanced(text):\n",
    "    text = clean_text(text)\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word.lower() not in stop_words and len(word) > 1])\n",
    "    return text\n",
    "\n",
    "def synonym_replacement(text, n):\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n: \n",
    "            break\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace(\"_\", \" \").lower()\n",
    "            if synonym != word:\n",
    "                synonyms.add(synonym)\n",
    "    return synonyms\n",
    "\n",
    "df['#1 String Cleaned'] = df['#1 String'].apply(clean_text)\n",
    "df['#2 String Cleaned'] = df['#2 String'].apply(clean_text)\n",
    "\n",
    "df['#1 String Processed'] = df['#1 String Cleaned'].apply(preprocess_text_advanced)\n",
    "df['#2 String Processed'] = df['#2 String Cleaned'].apply(preprocess_text_advanced)\n",
    "\n",
    "df_test['#1 String Cleaned'] = df_test['#1 String'].apply(clean_text)\n",
    "df_test['#2 String Cleaned'] = df_test['#2 String'].apply(clean_text)\n",
    "\n",
    "df_test['#1 String Processed'] = df_test['#1 String Cleaned'].apply(preprocess_text_advanced)\n",
    "df_test['#2 String Processed'] = df_test['#2 String Cleaned'].apply(preprocess_text_advanced)\n",
    "\n",
    "# Define a custom dataset for loading the data\n",
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text1 = row['#1 String Processed']\n",
    "        text2 = row['#2 String Processed']\n",
    "        label = int(row['Quality'])\n",
    "\n",
    "        # Apply synonym replacement augmentation\n",
    "        if random.random() < 0.5:  # 50% chance to apply augmentation\n",
    "            text1 = synonym_replacement(text1, 1)\n",
    "            text2 = synonym_replacement(text2, 1)\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text1,\n",
    "            text2,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize the RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = ParaphraseDataset(train_df, tokenizer, max_len=128)\n",
    "val_dataset = ParaphraseDataset(val_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Define the RoBERTa model with dropout and L2 regularization\n",
    "class ParaphraseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParaphraseModel, self).__init__()\n",
    "        self.bert = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Adding dropout layer with dropout rate of 0.3\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = self.dropout(outputs.logits)  # Apply dropout to logits\n",
    "        return outputs.loss, logits\n",
    "\n",
    "# Initialize model, loss function, and optimizer with L2 regularization\n",
    "model = ParaphraseModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Adding L2 regularization with weight decay\n",
    "\n",
    "# Scheduler for learning rate decay\n",
    "total_steps = len(train_loader) * 20\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop with early stopping\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "early_stopping_patience = 3\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer, scheduler, device, scaler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()  # Update the learning rate\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct_predictions.double() / len(data_loader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, np.mean(losses), f1\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct_predictions.double() / len(data_loader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, np.mean(losses), f1\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    train_acc, train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, scheduler, device, scaler)\n",
    "    print(f'Train loss: {train_loss}, Train accuracy: {train_acc}, Train F1: {train_f1}')\n",
    "\n",
    "    val_acc, val_loss, val_f1 = eval_model(model, val_loader, criterion, device)\n",
    "    print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}, Validation F1: {val_f1}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model_ROBERTA2.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model_ROBERTA2.pth'))\n",
    "\n",
    "# Save the model\n",
    "# torch.save(model.state_dict(), \"paraphrase_model_ROBERTA2.pth\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6af381-d6c0-4334-884a-a4a281093cd8",
   "metadata": {},
   "source": [
    "## Major Changes:\n",
    "### Mixed Precision Training: \n",
    "- Mixed precision training is a technique that uses both 16-bit (half-precision) and 32-bit (single-precision) floating-point numbers during training. This approach can speed up training and reduce memory usage, allowing you to train larger models or use larger batch sizes.\n",
    "    - 16-bit Floating Point (Half Precision): Uses less memory and can be processed faster by the GPU.\n",
    "    - 32-bit Floating Point (Single Precision): Provides more precision and is used where necessary to maintain numerical stability.\n",
    "- The GradScaler in PyTorch helps with the loss scaling part of mixed precision training. Here’s what it does:\n",
    "    - Scaling Up the Loss: Before the backward pass, the loss is multiplied by a scaling factor to prevent underflow of small gradient values.\n",
    "    - Backward Pass: Gradients are computed with the scaled loss.\n",
    "    - Unscaling the Gradients: After gradients are computed, they are divided by the scaling factor to return them to their correct scale.\n",
    "    - Checking for Overflow: GradScaler checks if any gradients are too large (overflow). If they are, the scaling factor is reduced to avoid instability in future iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e4785-7292-4a6a-bcda-e476d1f68130",
   "metadata": {},
   "source": [
    "## ROBERTA\n",
    "\n",
    "Main Differences Between BERT and RoBERTa\n",
    "- Training Data and Duration:\n",
    "    - BERT: Trained on the BooksCorpus and English Wikipedia (16GB of text data).\n",
    "    - RoBERTa: Trained on a much larger dataset (160GB), which includes BooksCorpus, English Wikipedia, CC-News, OpenWebText, and Stories. RoBERTa is trained longer and with more data, leading to better generalization.\n",
    "- Dynamic Masking:\n",
    "    - BERT: Uses static masking during pre-training, meaning that the same tokens are masked across different epochs.\n",
    "    - RoBERTa: Uses dynamic masking, where the tokens chosen for masking change with every epoch. This results in more robust training and better performance.\n",
    "\n",
    "**Masking**\\\n",
    "Masking is a technique used during the training of language models like BERT and RoBERTa. The idea is to randomly hide some words in a sentence and ask the model to predict these hidden (or masked) words based on the context provided by the other words in the sentence. This helps the model learn the relationships between words and their meanings within the context of a sentence.\n",
    "\n",
    "\n",
    "- Training Objectives:\n",
    "    - BERT: Utilizes the next sentence prediction (NSP) task during pre-training to predict if one sentence follows another.\n",
    "    - RoBERTa: Removes the NSP task, which was found to be less beneficial. Instead, it focuses on masked language modeling (MLM) with dynamic masking.\n",
    "- Hyperparameters:\n",
    "    - RoBERTa: Optimizes several hyperparameters such as batch size, learning rate, and training duration. These optimizations lead to more effective training and better performance.\n",
    "- Byte-Pair Encoding (BPE):\n",
    "    - RoBERTa: Uses byte-level BPE tokenization, which can handle rare and unseen words more effectively than the wordpiece tokenization used in BERT.\n",
    "    \n",
    "    \n",
    "**NSP**\\\n",
    "NSP is a task used during the pre-training phase of BERT (Bidirectional Encoder Representations from Transformers). The goal of NSP is to help the model understand the relationship between two sentences. \n",
    "- but RoBERTa found it less useful and removed it.\n",
    "\n",
    "**MLM**\n",
    "Masked Language Modeling (MLM) is a training objective used in both BERT and RoBERTa, where the model learns to predict missing words in a sentence.\n",
    "\n",
    "\n",
    "**BPE**\n",
    "Byte-Pair Encoding (BPE) is a tokenization method used to split text into subword units, which can handle rare and unseen words more effectively than traditional tokenization methods.\n",
    "- Purpose: To create a flexible and efficient vocabulary that can represent both common and rare words, reducing the number of unknown tokens and handling out-of-vocabulary words better.\n",
    "\n",
    "\n",
    "Why RoBERTa Might Perform Better\n",
    "- Larger and More Diverse Training Data: RoBERTa is trained on a significantly larger and more diverse dataset, allowing it to learn richer language representations and generalize better to various NLP tasks.\n",
    "- Dynamic Masking: The dynamic masking technique used by RoBERTa ensures that the model sees different masks of the same text during training, leading to a more robust understanding of the context and better performance.\n",
    "- Removal of NSP Task: By removing the next sentence prediction task, RoBERTa focuses entirely on the more beneficial masked language modeling objective, improving its performance.\n",
    "- Hyperparameter Tuning: RoBERTa benefits from extensive hyperparameter tuning, leading to more efficient training and better overall performance.\n",
    "- Byte-Level BPE: The use of byte-level BPE tokenization allows RoBERTa to handle a wider variety of text inputs, including those with rare or unseen words, more effectively than BERT.\n",
    "\n",
    "\n",
    "While BERT laid the foundation for many transformer-based models, RoBERTa improves upon it by addressing several limitations and introducing optimizations in training data, dynamic masking, and hyperparameters. These improvements make RoBERTa a more powerful and effective model for various NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcade5e1-2050-4f51-97fb-bb966eb294d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
