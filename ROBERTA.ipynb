{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "512817b1",
   "metadata": {},
   "source": [
    "## ROBERTA\n",
    "\n",
    "Main Differences Between BERT and RoBERTa\n",
    "- Training Data and Duration:\n",
    "    - BERT: Trained on the BooksCorpus and English Wikipedia (16GB of text data).\n",
    "    - RoBERTa: Trained on a much larger dataset (160GB), which includes BooksCorpus, English Wikipedia, CC-News, OpenWebText, and Stories. RoBERTa is trained longer and with more data, leading to better generalization.\n",
    "- Dynamic Masking:\n",
    "    - BERT: Uses static masking during pre-training, meaning that the same tokens are masked across different epochs.\n",
    "    - RoBERTa: Uses dynamic masking, where the tokens chosen for masking change with every epoch. This results in more robust training and better performance.\n",
    "\n",
    "**Masking**\\\n",
    "Masking is a technique used during the training of language models like BERT and RoBERTa. The idea is to randomly hide some words in a sentence and ask the model to predict these hidden (or masked) words based on the context provided by the other words in the sentence. This helps the model learn the relationships between words and their meanings within the context of a sentence.\n",
    "\n",
    "\n",
    "- Training Objectives:\n",
    "    - BERT: Utilizes the next sentence prediction (NSP) task during pre-training to predict if one sentence follows another.\n",
    "    - RoBERTa: Removes the NSP task, which was found to be less beneficial. Instead, it focuses on masked language modeling (MLM) with dynamic masking.\n",
    "- Hyperparameters:\n",
    "    - RoBERTa: Optimizes several hyperparameters such as batch size, learning rate, and training duration. These optimizations lead to more effective training and better performance.\n",
    "- Byte-Pair Encoding (BPE):\n",
    "    - RoBERTa: Uses byte-level BPE tokenization, which can handle rare and unseen words more effectively than the wordpiece tokenization used in BERT.\n",
    "    \n",
    "    \n",
    "**NSP**\\\n",
    "NSP is a task used during the pre-training phase of BERT (Bidirectional Encoder Representations from Transformers). The goal of NSP is to help the model understand the relationship between two sentences. \n",
    "- but RoBERTa found it less useful and removed it.\n",
    "\n",
    "**MLM**\n",
    "Masked Language Modeling (MLM) is a training objective used in both BERT and RoBERTa, where the model learns to predict missing words in a sentence.\n",
    "\n",
    "\n",
    "**BPE**\n",
    "Byte-Pair Encoding (BPE) is a tokenization method used to split text into subword units, which can handle rare and unseen words more effectively than traditional tokenization methods.\n",
    "- Purpose: To create a flexible and efficient vocabulary that can represent both common and rare words, reducing the number of unknown tokens and handling out-of-vocabulary words better.\n",
    "\n",
    "\n",
    "Why RoBERTa Might Perform Better\n",
    "- Larger and More Diverse Training Data: RoBERTa is trained on a significantly larger and more diverse dataset, allowing it to learn richer language representations and generalize better to various NLP tasks.\n",
    "- Dynamic Masking: The dynamic masking technique used by RoBERTa ensures that the model sees different masks of the same text during training, leading to a more robust understanding of the context and better performance.\n",
    "- Removal of NSP Task: By removing the next sentence prediction task, RoBERTa focuses entirely on the more beneficial masked language modeling objective, improving its performance.\n",
    "- Hyperparameter Tuning: RoBERTa benefits from extensive hyperparameter tuning, leading to more efficient training and better overall performance.\n",
    "- Byte-Level BPE: The use of byte-level BPE tokenization allows RoBERTa to handle a wider variety of text inputs, including those with rare or unseen words, more effectively than BERT.\n",
    "\n",
    "\n",
    "While BERT laid the foundation for many transformer-based models, RoBERTa improves upon it by addressing several limitations and introducing optimizations in training data, dynamic masking, and hyperparameters. These improvements make RoBERTa a more powerful and effective model for various NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89697940",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1dc52e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Loading data\n",
    "data = []\n",
    "with open('msr_paraphrase_train.txt', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        split_line = line.strip().split('\\t')\n",
    "        if len(split_line) == 5:\n",
    "            data.append(split_line)\n",
    "        else:\n",
    "            print(f\"Skipping line due to incorrect number of columns: {line}\")\n",
    "\n",
    "columns = [\"Quality\", \"#1 ID\", \"#2 ID\", \"#1 String\", \"#2 String\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df['Quality'] = df['Quality'].astype(int)\n",
    "\n",
    "data = []\n",
    "with open('msr_paraphrase_test.txt', 'r') as file:\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        split_line = line.strip().split('\\t')\n",
    "        if len(split_line) == 5:\n",
    "            data.append(split_line)\n",
    "        else:\n",
    "            print(f\"Skipping line due to incorrect number of columns: {line}\")\n",
    "\n",
    "df_test = pd.DataFrame(data, columns=columns)\n",
    "df_test['Quality'] = df_test['Quality'].astype(int)\n",
    "\n",
    "# Clean and preprocess data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text_advanced(text):\n",
    "    text = clean_text(text)\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word.lower() not in stop_words and len(word) > 1])\n",
    "    return text\n",
    "'''\n",
    "**Additional constraint: if word is less than 1 length long then do not include**\n",
    "\n",
    "Tokenization: Splits the text into individual words (tokens) using text.split().\n",
    "Stop Words Removal: Removes common stop words (e.g., \"and\", \"the\", \"is\") that do not contribute much meaning to the text. This is done by checking if each word is in the stop_words set.\n",
    "Lemmatization: Converts each word to its base form using a lemmatizer. For example, \"running\" becomes \"run\".\n",
    "Length Filter: Filters out any remaining words that are only one character long, as they are often not useful for understanding the text.\n",
    "'''\n",
    "\n",
    "\n",
    "df['#1 String Cleaned'] = df['#1 String'].apply(clean_text)\n",
    "df['#2 String Cleaned'] = df['#2 String'].apply(clean_text)\n",
    "\n",
    "df['#1 String Processed'] = df['#1 String Cleaned'].apply(preprocess_text_advanced)\n",
    "df['#2 String Processed'] = df['#2 String Cleaned'].apply(preprocess_text_advanced)\n",
    "\n",
    "df_test['#1 String Cleaned'] = df_test['#1 String'].apply(clean_text)\n",
    "df_test['#2 String Cleaned'] = df_test['#2 String'].apply(clean_text)\n",
    "\n",
    "df_test['#1 String Processed'] = df_test['#1 String Cleaned'].apply(preprocess_text_advanced)\n",
    "df_test['#2 String Processed'] = df_test['#2 String Cleaned'].apply(preprocess_text_advanced)\n",
    "\n",
    "\n",
    "# Define a custom dataset for loading the data\n",
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        text1 = row['#1 String Processed']\n",
    "        text2 = row['#2 String Processed']\n",
    "        label = int(row['Quality'])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text1,\n",
    "            text2,\n",
    "            max_length=self.max_len,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize the Roberta tokenizer (NEW WEEK 9 CHANGE)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = ParaphraseDataset(train_df, tokenizer, max_len=128)\n",
    "val_dataset = ParaphraseDataset(val_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5b763b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b201d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:38<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5905110976275276, Train accuracy: 0.6932515337423313, Train F1: 0.8032270759543487\n",
      "Validation loss: 0.46953992837784336, Validation accuracy: 0.7683823529411764, Validation F1: 0.8486789431545236\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.45524658094726356, Train accuracy: 0.7809815950920246, Train F1: 0.8381686310063464\n",
      "Validation loss: 0.43186574414664625, Validation accuracy: 0.803921568627451, Validation F1: 0.8697068403908795\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.34677621232820494, Train accuracy: 0.8496932515337424, Train F1: 0.8881278538812786\n",
      "Validation loss: 0.411208598929293, Validation accuracy: 0.8088235294117647, Validation F1: 0.8621908127208481\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.23716769330933982, Train accuracy: 0.9046012269938651, Train F1: 0.9287187714875087\n",
      "Validation loss: 0.4414789615308537, Validation accuracy: 0.8075980392156863, Validation F1: 0.8705688375927453\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.14927661959447114, Train accuracy: 0.9463190184049081, Train F1: 0.9598347486802846\n",
      "Validation loss: 0.5443666263611293, Validation accuracy: 0.8161764705882353, Validation F1: 0.8741610738255032\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10003535812903269, Train accuracy: 0.9668711656441719, Train F1: 0.9752633989922126\n",
      "Validation loss: 0.5559149718313825, Validation accuracy: 0.8174019607843137, Validation F1: 0.8738357324301439\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Define the BERT model\n",
    "class ParaphraseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParaphraseModel, self).__init__()\n",
    "        self.bert = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return outputs.loss, outputs.logits\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ParaphraseModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct_predictions.double() / len(data_loader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, np.mean(losses), f1\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct_predictions.double() / len(data_loader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, np.mean(losses), f1\n",
    "\n",
    "\n",
    "# Train the model with early stopping\n",
    "epochs = 15\n",
    "early_stopping_patience = 3\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    train_acc, train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Train loss: {train_loss}, Train accuracy: {train_acc}, Train F1: {train_f1}')\n",
    "\n",
    "    val_acc, val_loss, val_f1 = eval_model(model, val_loader, criterion, device)\n",
    "    print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}, Validation F1: {val_f1}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d70e4",
   "metadata": {},
   "source": [
    "## Part 1 Changes:\n",
    "### Mixed Precision Training: \n",
    "- Mixed precision training is a technique that uses both 16-bit (half-precision) and 32-bit (single-precision) floating-point numbers during training. This approach can speed up training and reduce memory usage, allowing you to train larger models or use larger batch sizes.\n",
    "    - 16-bit Floating Point (Half Precision): Uses less memory and can be processed faster by the GPU.\n",
    "    - 32-bit Floating Point (Single Precision): Provides more precision and is used where necessary to maintain numerical stability.\n",
    "- The GradScaler in PyTorch helps with the loss scaling part of mixed precision training. Here’s what it does:\n",
    "    - Scaling Up the Loss: Before the backward pass, the loss is multiplied by a scaling factor to prevent underflow of small gradient values.\n",
    "    - Backward Pass: Gradients are computed with the scaled loss.\n",
    "    - Unscaling the Gradients: After gradients are computed, they are divided by the scaling factor to return them to their correct scale.\n",
    "    - Checking for Overflow: GradScaler checks if any gradients are too large (overflow). If they are, the scaling factor is reduced to avoid instability in future iterations.\n",
    "    \n",
    "### More Training Epochs (15)\n",
    "\n",
    "### Early Stopping\n",
    "- If validation loss doesn't improve for 3 consecutive epochs, training is stopped. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe520d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212b1108",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:38<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6027219473731285, Train accuracy: 0.6530674846625767, Train F1: 0.7690422707780274\n",
      "Validation loss: 0.5372665132962021, Validation accuracy: 0.6850490196078431, Validation F1: 0.8108903605592348\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4845175618926684, Train accuracy: 0.7205521472392639, Train F1: 0.7914854657816434\n",
      "Validation loss: 0.4675552941420499, Validation accuracy: 0.7818627450980392, Validation F1: 0.8360957642725598\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.356283444546017, Train accuracy: 0.8015337423312884, Train F1: 0.8456215700310189\n",
      "Validation loss: 0.4495222001683478, Validation accuracy: 0.8174019607843137, Validation F1: 0.8634280476626948\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.24985294891338722, Train accuracy: 0.8542944785276074, Train F1: 0.8873606829499644\n",
      "Validation loss: 0.5691019771438018, Validation accuracy: 0.8051470588235294, Validation F1: 0.8604038630377524\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.16571562357392966, Train accuracy: 0.8803680981595092, Train F1: 0.9070543374642516\n",
      "Validation loss: 0.6240883675568244, Validation accuracy: 0.7781862745098039, Validation F1: 0.842745438748914\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1286497675612861, Train accuracy: 0.9015337423312884, Train F1: 0.9236623067776456\n",
      "Validation loss: 0.8471637606328609, Validation accuracy: 0.7781862745098039, Validation F1: 0.8413672217353199\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07559397115426905, Train accuracy: 0.9184049079754601, Train F1: 0.936787072243346\n",
      "Validation loss: 0.9356550476422497, Validation accuracy: 0.7904411764705882, Validation F1: 0.8544680851063831\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04393605685190243, Train accuracy: 0.9337423312883436, Train F1: 0.948936170212766\n",
      "Validation loss: 0.9629537802116543, Validation accuracy: 0.7916666666666666, Validation F1: 0.8519163763066202\n",
      "Early stopping triggered\n",
      "Fold 2\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.19175114891692704, Train accuracy: 0.8794848206071758, Train F1: 0.9065842643213691\n",
      "Validation loss: 0.02963030475246556, Validation accuracy: 0.996319018404908, Validation F1: 0.997270245677889\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1617940068244934, Train accuracy: 0.8948175406317082, Train F1: 0.9192371085472097\n",
      "Validation loss: 0.028804283789998174, Validation accuracy: 0.996319018404908, Validation F1: 0.997270245677889\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.14377047063088885, Train accuracy: 0.8923643054277829, Train F1: 0.9170800850460668\n",
      "Validation loss: 0.029822969380035706, Validation accuracy: 0.996319018404908, Validation F1: 0.997270245677889\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.12680603655091688, Train accuracy: 0.8994173566390679, Train F1: 0.9225684608120869\n",
      "Validation loss: 0.028754947305310006, Validation accuracy: 0.996319018404908, Validation F1: 0.997270245677889\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1196502710089964, Train accuracy: 0.8988040478380865, Train F1: 0.9219858156028369\n",
      "Validation loss: 0.02607779807465918, Validation accuracy: 0.996319018404908, Validation F1: 0.997270245677889\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0954735140845764, Train accuracy: 0.9098436062557498, Train F1: 0.9309210526315789\n",
      "Validation loss: 0.026733843052723243, Validation accuracy: 0.996319018404908, Validation F1: 0.997270245677889\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08783913342117824, Train accuracy: 0.9172033118675254, Train F1: 0.9365899483325505\n",
      "Validation loss: 0.026105094573223124, Validation accuracy: 0.9950920245398773, Validation F1: 0.9963570127504554\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07487900902097132, Train accuracy: 0.9144434222631095, Train F1: 0.933933222827374\n",
      "Validation loss: 0.025551463497857398, Validation accuracy: 0.9950920245398773, Validation F1: 0.9963570127504554\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07119275066161565, Train accuracy: 0.9168966574670347, Train F1: 0.9356447399667538\n",
      "Validation loss: 0.029220675625016585, Validation accuracy: 0.9938650306748467, Validation F1: 0.9954421148587055\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05594648261253229, Train accuracy: 0.9239497086783196, Train F1: 0.9413711583924349\n",
      "Validation loss: 0.029079523095039323, Validation accuracy: 0.9938650306748467, Validation F1: 0.9954421148587055\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05311593234849473, Train accuracy: 0.922723091076357, Train F1: 0.9402277039848198\n",
      "Validation loss: 0.0281890730382692, Validation accuracy: 0.9938650306748467, Validation F1: 0.9954504094631483\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04370704391861663, Train accuracy: 0.9282428702851886, Train F1: 0.9444971537001897\n",
      "Validation loss: 0.03355071325580973, Validation accuracy: 0.992638036809816, Validation F1: 0.9945255474452555\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04366045625989928, Train accuracy: 0.9325360318920577, Train F1: 0.9479905437352245\n",
      "Validation loss: 0.0321356680588888, Validation accuracy: 0.9938650306748467, Validation F1: 0.9954421148587055\n",
      "Early stopping triggered\n",
      "Fold 3\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04949260186221378, Train accuracy: 0.927936215884698, Train F1: 0.944378698224852\n",
      "Validation loss: 0.003063312345914835, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04454098756918136, Train accuracy: 0.9242563630788102, Train F1: 0.9410360467892098\n",
      "Validation loss: 0.0030213633298819117, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05130397311101357, Train accuracy: 0.9303894510886231, Train F1: 0.9462212745794836\n",
      "Validation loss: 0.0031268794201405755, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04608060580258276, Train accuracy: 0.9239497086783196, Train F1: 0.9411206077872745\n",
      "Validation loss: 0.002855642179401555, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04727727346414445, Train accuracy: 0.9371358478994174, Train F1: 0.9517987303080179\n",
      "Validation loss: 0.0027155185919528935, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04162067678921363, Train accuracy: 0.9297761422876418, Train F1: 0.9456702253855278\n",
      "Validation loss: 0.002802135102113015, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04095062726935116, Train accuracy: 0.9260962894817542, Train F1: 0.9427417438821573\n",
      "Validation loss: 0.0028413922843231144, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03715960944856645, Train accuracy: 0.9297761422876418, Train F1: 0.9456444338950866\n",
      "Validation loss: 0.002799240153228097, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03888330045247487, Train accuracy: 0.9349892670959828, Train F1: 0.949976403964134\n",
      "Validation loss: 0.0025659635829666226, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03847408543030421, Train accuracy: 0.9276295614842074, Train F1: 0.9439163498098859\n",
      "Validation loss: 0.00261912064731815, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.043553533327569456, Train accuracy: 0.9282428702851886, Train F1: 0.9443651925820257\n",
      "Validation loss: 0.0026534447633643067, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03547368989800852, Train accuracy: 0.9352959214964736, Train F1: 0.9502006136417276\n",
      "Validation loss: 0.002491680375647311, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03879579374174058, Train accuracy: 0.9153633854645815, Train F1: 0.9338763775754673\n",
      "Validation loss: 0.002381522023557302, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03924092290667342, Train accuracy: 0.9322293774915671, Train F1: 0.9477911646586344\n",
      "Validation loss: 0.0023362255157610657, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0351082413187981, Train accuracy: 0.9352959214964736, Train F1: 0.9500591715976332\n",
      "Validation loss: 0.0025311580530422576, Validation accuracy: 0.9987730061349693, Validation F1: 0.99909338168631\n",
      "Fold 4\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03685783972854123, Train accuracy: 0.9297761422876418, Train F1: 0.9456444338950866\n",
      "Validation loss: 0.001186574305928148, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.037839698390669974, Train accuracy: 0.9245630174793009, Train F1: 0.9414564493098524\n",
      "Validation loss: 0.001262471911853508, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03945545053255616, Train accuracy: 0.9294694878871512, Train F1: 0.9453941120607787\n",
      "Validation loss: 0.001211114419514642, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03365673307858992, Train accuracy: 0.933149340693039, Train F1: 0.9482676791646891\n",
      "Validation loss: 0.0011687065216749177, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03520348191480426, Train accuracy: 0.9310027598896045, Train F1: 0.9466445340289306\n",
      "Validation loss: 0.0011947567668268639, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.037437216981368905, Train accuracy: 0.927936215884698, Train F1: 0.9441141498216409\n",
      "Validation loss: 0.00121084751113884, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.037061993516616376, Train accuracy: 0.9328426862925483, Train F1: 0.9481902058197302\n",
      "Validation loss: 0.0012808940190748841, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.026725306595657385, Train accuracy: 0.9328426862925483, Train F1: 0.9480427046263346\n",
      "Validation loss: 0.0012320871670346927, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.030375874234938666, Train accuracy: 0.9273229070837167, Train F1: 0.9435579899976185\n",
      "Validation loss: 0.0012235510672040867, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Early stopping triggered\n",
      "Fold 5\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03848553529721411, Train accuracy: 0.9310027598896045, Train F1: 0.9467203409898177\n",
      "Validation loss: 0.0010594527531579574, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.032668428495526314, Train accuracy: 0.9386691199018706, Train F1: 0.9528301886792453\n",
      "Validation loss: 0.001060013825932116, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03321542721424325, Train accuracy: 0.9273229070837167, Train F1: 0.9437455494896748\n",
      "Validation loss: 0.001061405216151958, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02931436768495569, Train accuracy: 0.9356025758969642, Train F1: 0.9502840909090908\n",
      "Validation loss: 0.0010552534413542233, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03421972320396818, Train accuracy: 0.9303894510886231, Train F1: 0.9460166468489892\n",
      "Validation loss: 0.001053373461735307, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.036944697665817594, Train accuracy: 0.9417356639067771, Train F1: 0.9552730696798495\n",
      "Validation loss: 0.0010527408574525193, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.036829286475218465, Train accuracy: 0.9346826126954922, Train F1: 0.9495380241648899\n",
      "Validation loss: 0.0010495315837746888, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.029337958300340118, Train accuracy: 0.9316160686905858, Train F1: 0.9471188048375622\n",
      "Validation loss: 0.0010499233978015243, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.030674050959265408, Train accuracy: 0.9356025758969642, Train F1: 0.9502605400284226\n",
      "Validation loss: 0.0010445232767903921, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03222652769410143, Train accuracy: 0.9294694878871512, Train F1: 0.9454200284765069\n",
      "Validation loss: 0.0010490888985348684, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03783830381729001, Train accuracy: 0.9230297454768477, Train F1: 0.9402807518439209\n",
      "Validation loss: 0.0010521278737167664, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03210383593378698, Train accuracy: 0.927936215884698, Train F1: 0.9440609378719352\n",
      "Validation loss: 0.0010549974714533664, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.029536935167533217, Train accuracy: 0.9282428702851886, Train F1: 0.9442857142857143\n",
      "Validation loss: 0.001054519925531292, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [00:37<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.036226825954109504, Train accuracy: 0.9343759582950015, Train F1: 0.9493610979649788\n",
      "Validation loss: 0.001054619989065709, Validation accuracy: 1.0, Validation F1: 1.0\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Accuracy: 1.0\n",
      "Final Validation F1 Score: 1.0\n",
      "Final Validation Precision: 1.0\n",
      "Final Validation Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the BERT model\n",
    "class ParaphraseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParaphraseModel, self).__init__()\n",
    "        self.bert = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "        self.dropout = nn.Dropout(0.3)  # Adding dropout layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        pooled_output = self.dropout(outputs.logits)  # Apply dropout\n",
    "        return outputs.loss, pooled_output\n",
    "    \n",
    "'''\n",
    "- nn.Dropout(0.3): \n",
    "Adds a dropout layer with a 30% probability. \n",
    "This means that each neuron has a 30% chance of being zeroed \n",
    "out during training, helping the model generalize better.\n",
    "- optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01): \n",
    "Uses AdamW optimizer with weight decay. \n",
    "Weight decay adds a penalty for large weights, helping to prevent overfitting.\n",
    "'''\n",
    "\n",
    "\n",
    "# # Function to get synonyms for data augmentation\n",
    "# def get_synonyms(word):\n",
    "#     synonyms = set()\n",
    "#     for syn in wordnet.synsets(word):\n",
    "#         # Iterates over all synsets (groups of synonyms) of \n",
    "#         # the given word using the WordNet lexical database.\n",
    "#         for lemma in syn.lemmas():\n",
    "#             # Iterates over all lemmas (individual words) in each synset.\n",
    "#             synonyms.add(lemma.name())\n",
    "#                 # Adds each lemma (synonym) to the synonyms set. Using a\n",
    "#                 # set ensures that each synonym is unique.\n",
    "#     if word in synonyms:\n",
    "#         synonyms.remove(word)\n",
    "#         # Removes the original word from the synonyms set to avoid replacing a word with itself.\n",
    "#     return synonyms\n",
    "# '''\n",
    "# This helper function retrieves synonyms for \n",
    "# a given word using the WordNet lexical database.\n",
    "# '''\n",
    "\n",
    "\n",
    "\n",
    "# # Function for synonym replacement\n",
    "# def synonym_replacement(sentence, n):\n",
    "#     # Split the sentence into words\n",
    "#     words = sentence.split()\n",
    "#     # Create a copy of the sentence to modify\n",
    "#     new_sentence = words.copy()\n",
    "#     # Get a list of unique words that are not stopwords\n",
    "#     random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "#     random.shuffle(random_word_list)\n",
    "#     num_replaced = 0\n",
    "#     for random_word in random_word_list:\n",
    "#         synonyms = get_synonyms(random_word)\n",
    "#         if len(synonyms) >= 1:\n",
    "#             # Replace the word with a random synonym\n",
    "#             synonym = random.choice(list(synonyms))\n",
    "#             new_sentence = [synonym if word == random_word else word for word in new_sentence]\n",
    "#             num_replaced += 1\n",
    "#         if num_replaced >= n:\n",
    "#             break\n",
    "#     return ' '.join(new_sentence)\n",
    "\n",
    "# '''\n",
    "# This function replaces up to n words in a sentence with their synonyms.\n",
    "# '''\n",
    "\n",
    "# # Apply data augmentation\n",
    "# df['#1 String Augmented'] = df['#1 String Processed'].apply(lambda x: synonym_replacement(x, 2))\n",
    "# df['#2 String Augmented'] = df['#2 String Processed'].apply(lambda x: synonym_replacement(x, 2))\n",
    "\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = ParaphraseModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Added weight decay for regularization\n",
    "'''\n",
    " It works by adding a penalty to the loss function, which discourages the model from learning overly complex patterns that might not generalize well to unseen data.\n",
    "'''\n",
    "\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device, scaler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct_predictions.double() / len(data_loader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, np.mean(losses), f1\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct_predictions.double() / len(data_loader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, np.mean(losses), f1\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "best_model_state_dict = None\n",
    "best_fold = -1\n",
    "best_val_score = float('-inf')\n",
    "\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(df, df['Quality'])):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[val_index]\n",
    "\n",
    "    train_dataset = ParaphraseDataset(train_df, tokenizer, max_len=128)\n",
    "    val_dataset = ParaphraseDataset(val_df, tokenizer, max_len=128)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True)\n",
    "    \n",
    "    '''\n",
    "    scheduler: ReduceLROnPlateau is a learning rate scheduler that \n",
    "    reduces the learning rate when a metric has stopped improving. \n",
    "    - This helps to fine-tune the model more effectively by adapting the learning rate \n",
    "    based on the training performance.\n",
    "    '''\n",
    "            \n",
    "    early_stopping_patience = 5  # Increased patience for early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        train_acc, train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
    "        print(f'Train loss: {train_loss}, Train accuracy: {train_acc}, Train F1: {train_f1}')\n",
    "\n",
    "        val_acc, val_loss, val_f1 = eval_model(model, val_loader, criterion, device)\n",
    "        print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}, Validation F1: {val_f1}')\n",
    "        \n",
    "        scheduler.step(val_loss)  # Step the scheduler with the validation loss\n",
    "\n",
    "        # Early stopping based on validation loss improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_fold = fold\n",
    "            best_model_state_dict = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "                \n",
    "'''\n",
    "early_stopping_patience = 3: \n",
    "- Sets the number of epochs to wait before stopping training if no improvement is seen.\n",
    "'''\n",
    "              \n",
    "                            \n",
    "\n",
    "if best_model_state_dict:\n",
    "    torch.save(best_model_state_dict, 'best_model_fold.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a11e70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 0.9975467647960748\n",
      "Final Training Loss: 0.013625152233883045\n",
      "Final Training F1 Score: 0.9981851179673321\n",
      "Final Validation Accuracy: 1.0\n",
      "Final Validation Loss: 0.001054619989065709\n",
      "Final Validation F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "best_model = ParaphraseModel().to(device)\n",
    "best_model.load_state_dict(torch.load('best_model_fold.bin'))\n",
    "\n",
    "# Function to evaluate the model on a given data loader\n",
    "def eval_model_extended(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            \n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct_predictions.double() / len(data_loader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, np.mean(losses), f1\n",
    "\n",
    "# Evaluate the best model on the training and validation sets\n",
    "train_acc, train_loss, train_f1 = eval_model_extended(best_model, train_loader, device)\n",
    "val_acc, val_loss, val_f1 = eval_model_extended(best_model, val_loader, device)\n",
    "\n",
    "print(f'Final Training Accuracy: {train_acc}')\n",
    "print(f'Final Training Loss: {train_loss}')\n",
    "print(f'Final Training F1 Score: {train_f1}')\n",
    "print(f'Final Validation Accuracy: {val_acc}')\n",
    "print(f'Final Validation Loss: {val_loss}')\n",
    "print(f'Final Validation F1 Score: {val_f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2cdae0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2 Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005929e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Regularization\n",
    "- Regularization techniques help prevent overfitting. Dropout randomly sets a fraction of input units to 0 at each update during training time, which helps prevent over-reliance on specific neurons. Weight decay (L2 regularization) adds a penalty to the loss function based on the size of the weights.\n",
    "\n",
    "#### Dropout\n",
    "- Overfitting Problem: In a neural network, overfitting occurs when the model learns to perform well on the training data but fails to generalize to unseen data. This typically happens when the model relies too heavily on specific neurons or learns noise in the training data.\n",
    "    - Dropout Solution: By randomly ignoring neurons during training, dropout forces the network to learn redundant representations and not rely on any specific neurons. This helps the model generalize better to new data.\n",
    "- Robust Features: The model learns to distribute the learning across all neurons, ensuring that the absence of any particular neuron does not significantly impact the network's performance.\n",
    "    - Redundant Representations: Dropout encourages the creation of redundant features, which means the network learns multiple ways to represent the same information. This redundancy makes the model more robust to changes and variations in the input data.\n",
    "\n",
    "#### Weight Decay (L2 Regularization)\n",
    "- Weights are numerical values that determine the strength of the connections between neurons (or nodes) in adjacent layers of a neural network.\n",
    "- They adjust the input signals in each neuron and control how much influence each input will have on the neuron's output.\n",
    "\n",
    "- Overfitting Problem: Similar to dropout, overfitting can occur when the model's weights become too large, allowing it to memorize the training data instead of learning general patterns.\n",
    "    - Weight Decay Solution: By penalizing large weights, weight decay encourages the model to keep its weights small, which helps in reducing the model's capacity to memorize the training data. This leads to better generalization to unseen data.\n",
    "- Simplicity and Generalization: Models with smaller weights are generally simpler and more likely to generalize well to new data. Weight decay helps in promoting simplicity by discouraging overly complex models with large weights.\n",
    "\n",
    "### 3. Learning Rate Scheduling (ReduceLROnPlateau)\n",
    "- Learning rate scheduling dynamically adjusts the learning rate during training, which helps the model converge more effectively. \n",
    "- It reduces the learning rate only when the training process stagnates, allowing the model to continue learning at a higher rate when it is still improving\n",
    "\n",
    "Learning Rates:\n",
    "- A high learning rate means large updates to the weights, which can speed up learning but might overshoot the optimal solution.\n",
    "- A low learning rate means small updates to the weights, which allows for more precise adjustments but can make learning slow.\n",
    "\n",
    "#### Adaptive Learning Rate:\n",
    "- Why: The ReduceLROnPlateau scheduler adjusts the learning rate based on the actual performance of the model rather than following a fixed schedule.\n",
    "- How: It reduces the learning rate only when the training process stagnates, allowing the model to continue learning at a higher rate when it is still improving.\n",
    "#### Handling Plateaus:\n",
    "- Why: Training often encounters plateaus where the loss or accuracy does not improve for several epochs.\n",
    "- How: ReduceLROnPlateau helps in overcoming these plateaus by reducing the learning rate, which can help the model find new, better minima in the loss landscape.\n",
    "#### Prevention of Overshooting:\n",
    "- Why: A linear scheduler reduces the learning rate uniformly over time, which might not be optimal for all stages of training.\n",
    "- How: By adapting the learning rate based on performance, ReduceLROnPlateau prevents the learning rate from becoming too small too early, which can hinder training progress.\n",
    "#### Fine-tuning:\n",
    "- Why: As training progresses, smaller learning rates are needed to make fine adjustments to the weights.\n",
    "- How: ReduceLROnPlateau automatically decreases the learning rate when fine-tuning is necessary, ensuring better convergence and finer adjustments.\n",
    "\n",
    "    \n",
    "    \n",
    "### 4. Cross-Validation\n",
    "- Cross-validation is a technique used to evaluate the performance of a model by partitioning the data into several subsets (folds) and training/testing the model multiple times, each time using a different subset as the validation set and the remaining subsets as the training set. This helps in assessing the model's ability to generalize to unseen data.\n",
    "\n",
    "Stratified K Fold:\n",
    "- Splits the dataset into k folds (subsets), ensuring that each fold has a representative distribution of the target variable.\n",
    "- For example, in a 5-fold cross-validation:\n",
    "    - Fold 1: Train on Folds 2-5, Test on Fold 1\n",
    "    - Fold 2: Train on Folds 1, 3-5, Test on Fold 2\n",
    "    - And so on...\n",
    "#### Reliable Performance Estimate:\n",
    "- Cross-validation provides a more reliable estimate of model performance compared to a single train-test split.\n",
    "- How: By averaging the performance across multiple folds, it reduces the impact of variance and provides a clearer picture of how the model will generalize to new data.\n",
    "#### Detection of Overfitting:\n",
    "- Helps identify if the model is overfitting to a particular subset of the data.\n",
    "- How: Each fold uses a different validation set, so consistent performance across folds indicates good generalization.\n",
    "#### Robust Model Evaluation:\n",
    "- Ensures that the model's performance is not dependent on a specific train-test split.\n",
    "- How: By training and validating the model on different subsets, it tests the model's robustness to variations in the data.\n",
    "#### Efficient Use of Data:\n",
    "- Maximizes the use of available data for training and validation.\n",
    "- How: Each data point is used for both training and validation across different folds, leading to a more efficient use of the dataset.\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26be19e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### RAW CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw code: \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the BERT model\n",
    "class ParaphraseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParaphraseModel, self).__init__()\n",
    "        self.bert = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "        self.dropout = nn.Dropout(0.3)  # Adding dropout layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        pooled_output = self.dropout(outputs.logits)  # Apply dropout\n",
    "        return outputs.loss, pooled_output\n",
    "model = ParaphraseModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Added weight decay for regularization\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device, scaler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for data in tqdm(data_loader):\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct_predictions.double() / len(data_loader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, np.mean(losses), f1\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "\n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct_predictions.double() / len(data_loader.dataset)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    return acc, np.mean(losses), f1\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "best_model_state_dict = None\n",
    "best_fold = -1\n",
    "best_val_score = float('-inf')\n",
    "\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(df, df['Quality'])):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[val_index]\n",
    "\n",
    "    train_dataset = ParaphraseDataset(train_df, tokenizer, max_len=128)\n",
    "    val_dataset = ParaphraseDataset(val_df, tokenizer, max_len=128)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    # model = ParaphraseModel().to(device)\n",
    "    # optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "            \n",
    "    early_stopping_patience = 5  # Increased patience for early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        train_acc, train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
    "        print(f'Train loss: {train_loss}, Train accuracy: {train_acc}, Train F1: {train_f1}')\n",
    "\n",
    "        val_acc, val_loss, val_f1 = eval_model(model, val_loader, criterion, device)\n",
    "        print(f'Validation loss: {val_loss}, Validation accuracy: {val_acc}, Validation F1: {val_f1}')\n",
    "        \n",
    "        scheduler.step(val_loss)  # Step the scheduler with the validation loss\n",
    "\n",
    "        # Early stopping based on validation loss improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_fold = fold\n",
    "            best_model_state_dict = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "          \n",
    "                            \n",
    "\n",
    "if best_model_state_dict:\n",
    "    torch.save(best_model_state_dict, 'best_model_fold.bin')\n",
    "    \n",
    "best_model = ParaphraseModel().to(device)\n",
    "best_model.load_state_dict(torch.load('best_model_fold.bin'))\n",
    "\n",
    "# Function to evaluate the model on the validation set\n",
    "def final_eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            labels = data['labels'].to(device)\n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_ascore(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    return acc, f1, precision, recall\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "val_acc, val_f1, val_precision, val_recall = final_eval_model(best_model, val_loader, device)\n",
    "print(f'Final Validation Accuracy: {val_acc}')\n",
    "print(f'Final Validation F1 Score: {val_f1}')\n",
    "print(f'Final Validation Precision: {val_precision}')\n",
    "print(f'Final Validation Recall: {val_recall}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
